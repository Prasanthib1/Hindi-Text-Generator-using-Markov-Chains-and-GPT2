{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi_Text_Generator_using_GPT_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddiOCukhoJAq"
      },
      "source": [
        "# Hindi Text Generator Using GPT-2\n",
        "\n",
        "## Team - 15 \n",
        "* _Praneeth G_\n",
        "* _Rishik TS_\n",
        "* _Prashanti B_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuKDAEmftde"
      },
      "source": [
        "# Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itaZgyWPfRAO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "#/content/drive/MyDrive/Colab Notebooks/Datafiles/GPT2_HINDI_TEXT_GENERATOR\n",
        "# lang = 'bn'\n",
        "\n",
        "def store(corpus, lang):\n",
        "    base_path = \"/content/drive/MyDrive/Colab Notebooks/Datafiles/GPT2_HINDI_TEXT_GENERATOR\"\n",
        "    store_path = os.path.join(base_path, '{}_corpus'.format(lang))\n",
        "    if not os.path.exists(store_path):\n",
        "        os.mkdir(store_path)\n",
        "    file_idx=1\n",
        "    for text in corpus.get_texts():\n",
        "        current_file_path = os.path.join(store_path, 'article_{}.txt'.format(file_idx))\n",
        "        with open(current_file_path, 'w' , encoding='utf-8') as file:\n",
        "            file.write(bytes(' '.join(text), 'utf-8').decode('utf-8'))\n",
        "        #endwith\n",
        "        file_idx += 1\n",
        "    #endfor\n",
        "\n",
        "def tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list:\n",
        "    return [token for token in text.split() if token_min_len <= len(token) <= token_max_len]\n",
        "\n",
        "def run(lang):\n",
        "    origin='https://dumps.wikimedia.org/{}wiki/latest/{}wiki-latest-pages-articles.xml.bz2'.format(lang,lang)\n",
        "    fname='{}wiki-latest-pages-articles.xml.bz2'.format(lang)\n",
        "    file_path = tf.keras.utils.get_file(origin=origin, fname=fname, untar=False, extract=False)\n",
        "    corpus = WikiCorpus(file_path,  lower=False, tokenizer_func=tokenizer_func)\n",
        "    store(corpus, lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wng1f2J9fyaO"
      },
      "source": [
        "run(\"hi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCc-1ejWgaf0"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyyGPo73f3sw"
      },
      "source": [
        "% cd ./drive/MyDrive/Colab Notebooks/Datafiles/GPT2_HINDI_TEXT_GENERATOR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdtYISKDf7AP"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaHz0sj3gBBV"
      },
      "source": [
        "Installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUC9B7Taf96m"
      },
      "source": [
        "!pip3 install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjPzDwAgG74"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czaz8TeziEfb"
      },
      "source": [
        "Getting the articles Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CibpYKwPghZh"
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "# the folder 'text' contains all the files\n",
        "paths = [str(x) for x in Path(\"./hi_corpus/\").glob(\"**/*.txt\")]\n",
        "paths = paths[:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGZsoqaLiRaB"
      },
      "source": [
        "Storing the stored Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx1rpcpZgmLq"
      },
      "source": [
        "for filename in paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "   x = f.read()\n",
        "   x = x.replace(\"'\", \"\")\n",
        "   x = x.replace(\"=\", \"\")\n",
        "  with open(\"./cleaned_hi_corpus/\"+filename[10:],\"w\",encoding='utf-8') as f:\n",
        "    f.write(x)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lLEV9k5iYGy"
      },
      "source": [
        "# Tokenising the Data Initialising the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eOqjgWpgopN"
      },
      "source": [
        "from tokenise import BPE_token\n",
        "from pathlib import Path\n",
        "import os\n",
        "# the folder 'text' contains all the files\n",
        "new_paths = [str(x) for x in Path(\"./cleaned_hi_corpus/\").glob(\"**/*.txt\")]\n",
        "#print(paths)\n",
        "tokenizer = BPE_token()\n",
        "# train the tokenizer model\n",
        "tokenizer.bpe_train(new_paths)\n",
        "# saving the tokenized data in our specified folder \n",
        "save_path = 'cleaned_tokenized_data'\n",
        "tokenizer.save_tokenizer(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie1f_B5AgrMY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.vocab_size,\n",
        "  bos_token_id=tokenizer.bos_token_id,\n",
        "  eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "# creating the model\n",
        "model = TFGPT2LMHeadModel(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maWDZ96Sil5f"
      },
      "source": [
        "Combining the whole data to single string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olBHjrpbgtsM"
      },
      "source": [
        "single_string = ''\n",
        "for filename in new_paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "   x = f.read()\n",
        "   x = x.replace(\"'\", \"\")\n",
        "   x = x.replace(\"=\", \"\")\n",
        "  single_string += x + tokenizer.eos_token\n",
        "string_tokenized = tokenizer.encode(single_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83oPJb5oiwJo"
      },
      "source": [
        "Batching and making the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWU1_1CBhDka"
      },
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1pda9TXizw4"
      },
      "source": [
        "# Compiling and Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry7nPtFkhGLL"
      },
      "source": [
        "# defining our optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer,loss=[loss, *[None]* model.config.n_layer],metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7vgxxyUjJxS"
      },
      "source": [
        "Fitting the dataset to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zYjO3aThUEW"
      },
      "source": [
        "num_epoch = 25\n",
        "#new_dataset = list(dataset.as_numpy_iterator())\n",
        "history = model.fit(dataset, epochs=num_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB96Exd_hhvb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAb9q2K6hWm3"
      },
      "source": [
        "model.save_pretrained(\"./Models/wiki_300_gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kcILtjGhYtM"
      },
      "source": [
        "!ls ./Models/wiki_300_gpt2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtE4w_PjNNF"
      },
      "source": [
        "Plotting the evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaMWy92gha3r"
      },
      "source": [
        "def plot(history):\n",
        "    \n",
        "    plt.plot(history.history['logits_accuracy'])\n",
        "    #plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "    plt.title('model accuracy and loss')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train'], loc='upper left')\n",
        "    plt.savefig(\"./Models/wiki_300_gpt2/\"+'model_acc.png')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['logits_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['loss', 'logits_loss'], loc='upper right')\n",
        "    plt.savefig(\"./Models/wiki_300_gpt2/\"+'model_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['logits_accuracy'])\n",
        "    plt.plot(history.history['logits_loss'])\n",
        "    plt.title('model evaluation')\n",
        "    plt.ylabel('logits values')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.legend(['logits_accuracy', 'logits_loss'], loc='upper right')\n",
        "    plt.savefig(\"./Models/wiki_300_gpt2/\"+'model_acc_loss.png')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j_dQCXGhdQr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTYnNY2IjRVP"
      },
      "source": [
        "# Generating the Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lUs-1_WhoPc"
      },
      "source": [
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQAAfBlohqjT"
      },
      "source": [
        "def generate_text(text,model,max_length):\n",
        "  # encoding the input text\n",
        "  input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "  # getting out output\n",
        "  beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length = max_length,\n",
        "    num_beams = 8,\n",
        "    temperature = 0.8,\n",
        "    no_repeat_ngram_size=3,\n",
        "    num_return_sequences=3\n",
        "  )\n",
        "\n",
        "  output_text = tokenizer.decode(beam_output[0])\n",
        "  return output_text\n",
        "\n",
        "output_dir = \"cleaned_tokenized_data\"\n",
        "model_dir = \"./Models/wiki_300_gpt2/\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "model = TFGPT2LMHeadModel.from_pretrained(model_dir)\n",
        "\n",
        "text = \"आंध्रप्रदेश की संस्कृति\"\n",
        "print(\"Given text: \",text)\n",
        "\n",
        "output_text = generate_text(text,model,60)\n",
        "file1 = open(\"output.txt\",\"w\")#write mode\n",
        "file1.write(output_text)\n",
        "file1.close()\n",
        "print(\"Generated text: \",output_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}